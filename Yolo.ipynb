{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKdcvMtSMpSg"
   },
   "source": [
    "#Майнор ИАД. Домашнее задание 3. YOLO.\n",
    "\n",
    "В этом задании вы напишете и обучите свой собственный YOLO детектор. Нужно будет разобраться со статьей: понять какого формата должна быть обучающая пара (x, y), как перевести лосс из математической формулы в питоновский код - ну и конечно понять и реализовать саму архитектуру модели.\n",
    "\n",
    "Выборка на котрой мы будем обучать модель состоит из разнообразных фотографий яблок, бананов и апельсинов. Данные скачиваем [отсюда](https://drive.google.com/file/d/1d8GSfZoWbraWCSUhX78yl4CnMFYE-5n3/view?usp=sharing).\n",
    "\n",
    "Баллы за ДЗ распределены следующим образом: \n",
    "- Выборка для YoloV1 - 2 балла\n",
    "- YOLO модель - 2 балла\n",
    "- YOLO Loss - 3 балла\n",
    "- Вспомогательные функции - 2 балла\n",
    "- Обучение и расчет метрик - 2 балла\n",
    "\n",
    "Для построения и обучения можно использовать как pytorch, так и pytorch-lightning.\n",
    "\n",
    "Да-да, баллов в сумме получается 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNSQ7FNss30F"
   },
   "source": [
    "Скачаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lN1dE6eY7PjV",
    "outputId": "beb005ec-2b9b-42ab-c294-127ee4ec4b39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace __MACOSX/._data? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n",
      "total 56\n",
      "drwxr-xr-x  4 kirill  staff    128 Nov 28 09:32 \u001b[34m__MACOSX\u001b[m\u001b[m\n",
      "drwx------  5 kirill  staff    160 Nov 26 11:32 \u001b[34mdata\u001b[m\u001b[m\n",
      "-rw-r--r--@ 1 kirill  staff  27878 Dec  2 19:05 hw3.ipynb\n"
     ]
    }
   ],
   "source": [
    "!wget --quiet --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://drive.google.com/uc?export=download&id=1d8GSfZoWbraWCSUhX78yl4CnMFYE-5n3' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1d8GSfZoWbraWCSUhX78yl4CnMFYE-5n3\" -O data.zip && rm -rf /tmp/cookies.txt\n",
    "!unzip -q data.zip\n",
    "!rm data.zip\n",
    "!ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ep38vdW_s-Rz"
   },
   "source": [
    "Посмотрим как выглядит один из файлов разметки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OqwaHl3ntBaN",
    "outputId": "77ce5c63-e716-4a57-90a9-2179534fa789"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<annotation>\n",
      "\t<folder>train</folder>\n",
      "\t<filename>apple_3.jpg</filename>\n",
      "\t<path>C:\\tensorflow1\\models\\research\\object_detection\\images\\train\\apple_3.jpg</path>\n",
      "\t<source>\n",
      "\t\t<database>Unknown</database>\n",
      "\t</source>\n",
      "\t<size>\n",
      "\t\t<width>1000</width>\n",
      "\t\t<height>708</height>\n",
      "\t\t<depth>3</depth>\n",
      "\t</size>\n",
      "\t<segmented>0</segmented>\n",
      "\t<object>\n",
      "\t\t<name>apple</name>\n",
      "\t\t<pose>Unspecified</pose>\n",
      "\t\t<truncated>1</truncated>\n",
      "\t\t<difficult>0</difficult>\n",
      "\t\t<bndbox>\n",
      "\t\t\t<xmin>584</xmin>\n",
      "\t\t\t<ymin>438</ymin>\n",
      "\t\t\t<xmax>867</xmax>\n",
      "\t\t\t<ymax>708</ymax>\n",
      "\t\t</bndbox>\n",
      "\t</object>\n",
      "\t<object>\n",
      "\t\t<name>apple</name>\n",
      "\t\t<pose>Unspecified</pose>\n",
      "\t\t<truncated>0</truncated>\n",
      "\t\t<difficult>0</difficult>\n",
      "\t\t<bndbox>\n",
      "\t\t\t<xmin>492</xmin>\n",
      "\t\t\t<ymin>141</ymin>\n",
      "\t\t\t<xmax>740</xmax>\n",
      "\t\t\t<ymax>394</ymax>\n",
      "\t\t</bndbox>\n",
      "\t</object>\n",
      "\t<object>\n",
      "\t\t<name>apple</name>\n",
      "\t\t<pose>Unspecified</pose>\n",
      "\t\t<truncated>0</truncated>\n",
      "\t\t<difficult>0</difficult>\n",
      "\t\t<bndbox>\n",
      "\t\t\t<xmin>176</xmin>\n",
      "\t\t\t<ymin>199</ymin>\n",
      "\t\t\t<xmax>490</xmax>\n",
      "\t\t\t<ymax>466</ymax>\n",
      "\t\t</bndbox>\n",
      "\t</object>\n",
      "\t<object>\n",
      "\t\t<name>apple</name>\n",
      "\t\t<pose>Unspecified</pose>\n",
      "\t\t<truncated>0</truncated>\n",
      "\t\t<difficult>0</difficult>\n",
      "\t\t<bndbox>\n",
      "\t\t\t<xmin>367</xmin>\n",
      "\t\t\t<ymin>17</ymin>\n",
      "\t\t\t<xmax>619</xmax>\n",
      "\t\t\t<ymax>240</ymax>\n",
      "\t\t</bndbox>\n",
      "\t</object>\n",
      "\t<object>\n",
      "\t\t<name>apple</name>\n",
      "\t\t<pose>Unspecified</pose>\n",
      "\t\t<truncated>0</truncated>\n",
      "\t\t<difficult>0</difficult>\n",
      "\t\t<bndbox>\n",
      "\t\t\t<xmin>642</xmin>\n",
      "\t\t\t<ymin>35</ymin>\n",
      "\t\t\t<xmax>907</xmax>\n",
      "\t\t\t<ymax>269</ymax>\n",
      "\t\t</bndbox>\n",
      "\t</object>\n",
      "</annotation>\n"
     ]
    }
   ],
   "source": [
    "!cat data/train/apple_3.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdQxrA5_MpSg"
   },
   "source": [
    "## Релизуйте выборку для YoloV1 - 2 балла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "QXG9reop-BkS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import glob\n",
    "import tqdm\n",
    "import xmltodict\n",
    "\n",
    "from IPython.core.display import struct\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "import albumentations as A\n",
    "import albumentations.pytorch\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "# Добавьте необходимые вам библиотеки, если их не окажется в списке выше\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gL8_CyyTYJ-"
   },
   "source": [
    "Так как в этом домашнем задании использовать аугментации для обучения __обязательно__ - советуем воспользоваться библиотекой albumentations.\n",
    "\n",
    "Она  особенно удобна, поскольку умеет сама вычислять новые координаты bounding box'ов после трансформаций картинки. Для знакомства с этим механизмом советуем следующий [гайд](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/). \n",
    "\n",
    "Вы все еще можете избрать путь torchvision.transforms, вам потребуется знакомый нам метод `__getitem__`, однако вычислять новые координаты bounding box'ов после трансформаций вам придётся вручную\n",
    "\n",
    "__Обратите внимание__ на то, что в статье коробки предсказаний параметризуются через: _(x_center, y_center, width, height)_ (причем эти значения _относительные_), а в наших файлах - это _(x_min, y_min, x_max, y_max)_\n",
    "\n",
    "Также, помните что модель должна предсказывать как прямоугольник с обьектом, так и вероятности каждого класса!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2tag = {\"apple\": 1, \"orange\": 2, \"banana\": 3}\n",
    "\n",
    "class FruitDataset(Dataset):\n",
    "    def __init__(self, data_dir, transforms=None):\n",
    "        self.image_paths = glob.glob( os.path.join(data_dir,'*.jpg'))\n",
    "        self.box_paths = [x+\".xml\" for x in [os.path.splitext(x)[0] for x in self.image_paths]]\n",
    "        assert len(self.image_paths) == len(self.box_paths)\n",
    "\n",
    "        self.transforms = transforms\n",
    "        self.S = 7\n",
    "        self.B = 2\n",
    "        self.C = 3\n",
    "\n",
    "        \n",
    "\n",
    "    # Координаты прямоугольников советуем вернуть именно в формате (x_center, y_center, width, height)\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.array(Image.open(self.image_paths[idx]).convert(\"RGB\"))\n",
    "        boxes, class_labels = self.__get_boxes_from_xml(self.box_paths[idx],image)\n",
    "\n",
    "\n",
    "        if self.transforms:\n",
    "          augm = self.transforms(image=image, bboxes = boxes, class_labels=class_labels)\n",
    "          image = augm['image']\n",
    "          boxes = augm['bboxes']\n",
    "          class_labels = augm['class_labels']\n",
    "\n",
    "        else:\n",
    "            image = torch.Tensor(image)\n",
    "            boxes = torch.Tensor(boxes)        \n",
    "\n",
    "        \n",
    "        return image, (boxes, class_labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __get_boxes_from_xml(self, xml_filename: str,width,height):\n",
    "      \"\"\"\n",
    "      Метод, который считает и распарсит (с помощью xmltodict) переданный xml\n",
    "      файл и вернет координаты прямоугольников обьектов на соответсвующей фотографии\n",
    "      и название класса обьекта в каждом прямоугольнике\n",
    "\n",
    "      Обратите внимание, что обьектов может быть как несколько, так и один единственный\n",
    "      \"\"\"\n",
    "      boxes = []\n",
    "      class_labels = []\n",
    "      dictionary = xmltodict.parse(open(xml_filename,\"r\").read())['annotation']\n",
    "\n",
    "      with open(xml_filename) as f:\n",
    "            dict_of_bbox = xmltodict.parse(f.read())\n",
    "\n",
    "      if type(dict_of_bbox['annotation']['object']) is list:\n",
    "            for obj in dict_of_bbox['annotation']['object']:\n",
    "                box = [int(num) for num in list(obj['bndbox'].values())]\n",
    "                yolo_box = self.__convert_to_yolo_box_params(box, width, height)\n",
    "                boxes += [yolo_box]\n",
    "                class_labels += [class2tag[obj['name']]]\n",
    "        \n",
    "      else:\n",
    "            box = [int(num) for num in  list(dict_of_bbox['annotation']['object']['bndbox'].values())]\n",
    "            yolo_box = self.__convert_to_yolo_box_params(box, width, height)\n",
    "            boxes += [yolo_box]\n",
    "            class_labels += [class2tag[dict_of_bbox['annotation']['object']['name']]]\n",
    "      \n",
    "      return boxes,class_labels\n",
    "\n",
    "    def __convert_to_yolo_box_params(self, box_coordinates: List[int], im_w, im_h):\n",
    "      \"\"\"\n",
    "      Перейти от [xmin, ymin, xmax, ymax] к [x_center, y_center, width, height].\n",
    "      \n",
    "      Обратите внимание, что параметры [x_center, y_center, width, height] - это\n",
    "      относительные значение в отрезке [0, 1]\n",
    "\n",
    "      :param: box_coordinates - координаты коробки в формате [xmin, ymin, xmax, ymax]\n",
    "      :param: im_w - ширина исходного изображения\n",
    "      :param: im_h - высота исходного изображения\n",
    "\n",
    "      :return: координаты коробки в формате [x_center, y_center, width, height]\n",
    "      \"\"\"\n",
    "      ans = [] \n",
    "      ans.append((box_coordinates[0] + box_coordinates[2]) / 2 / im_w)# x_center  \n",
    "      ans.append((box_coordinates[1] + box_coordinates[3]) / 2 / im_h)  # y_center\n",
    "      ans.append((box_coordinates[2] - box_coordinates[0]) / im_w)# width  \n",
    "      ans.append((box_coordinates[3] - box_coordinates[1]) / im_h)  # height \n",
    "      return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "OwXeSiAjdGeq"
   },
   "outputs": [],
   "source": [
    "WIDTH, HEIGHT = 448, 448\n",
    "\n",
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Resize(WIDTH, HEIGHT),\n",
    "     A.pytorch.transforms.ToTensorV2()\n",
    "                             \n",
    "                             \n",
    "                             \n",
    "],\n",
    "                             bbox_params=A.BboxParams(format='yolo',\n",
    "                                                      label_fields=['class_labels']))\n",
    "test_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "     A.pytorch.transforms.ToTensorV2()\n",
    "], \n",
    "                            bbox_params=A.BboxParams(format='yolo',\n",
    "                                                     label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH, HEIGHT = 448, 448\n",
    "\n",
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "train_transform = A.Compose([A.Resize(HEIGHT, WIDTH),\n",
    "                             A.OneOf([\n",
    "                                 A.HorizontalFlip(p=1),\n",
    "                                 A.RandomRotate90(p=1),\n",
    "                                 A.VerticalFlip(p=1)\n",
    "                                 ], p=0.5),\n",
    "                             A.OneOf([\n",
    "                                 A.RandomBrightnessContrast(p=1),\n",
    "                                 A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05, p=1),\n",
    "                                 A.ToGray (p=1)\n",
    "                                 ], p=0.3),\n",
    "                             A.OneOf([\n",
    "                                     A.Blur(blur_limit=(7 ,11), p=1),\n",
    "                                     A.MedianBlur(blur_limit=(7 ,11), p=1)\n",
    "                                     ], p=0.2),\n",
    "                             A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "                             ToTensorV2()],\n",
    "                             bbox_params=A.BboxParams(format='yolo',\n",
    "                                                      label_fields=['class_labels']))\n",
    "test_transform = A.Compose([A.Resize(HEIGHT, WIDTH),\n",
    "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "                            ToTensorV2()],\n",
    "                            bbox_params=A.BboxParams(format='yolo',\n",
    "                                                     label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ayPwbRKocdCE",
    "outputId": "c7ec7134-c763-435c-a575-de2e16122a82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тесты успешно пройдены\n"
     ]
    }
   ],
   "source": [
    "train_dataset = FruitDataset(\n",
    "    transforms=train_transform,\n",
    "    data_dir=\"./data/train\"\n",
    "    )\n",
    "\n",
    "val_dataset = FruitDataset(\n",
    "    transforms=test_transform, \n",
    "    data_dir=\"./data/test\"\n",
    "    )\n",
    "\n",
    "# Немного проверок, чтобы убедиться в правильности направления решения\n",
    "assert isinstance(train_dataset[0], tuple)\n",
    "assert len(train_dataset[0]) == 2\n",
    "assert isinstance(train_dataset[0][0], torch.Tensor)\n",
    "assert isinstance(train_dataset[0][1], tuple)\n",
    "assert len(train_dataset[0][1]) == 2\n",
    "print(\"Тесты успешно пройдены\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "9V1Tl_GAdeIv"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size= 4,\n",
    "    shuffle=True)\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=4, \n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fRR9ns6MpSh"
   },
   "source": [
    "Теперь определим функцию для рассчета Intersection Over Union по 4 углам двух прямоугольников"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "Rd88hnZiMpSh"
   },
   "outputs": [],
   "source": [
    "def intersection_over_union(predicted_bbox, gt_bbox) -> float:\n",
    "    \"\"\"\n",
    "    Intersection Over Union для двух прямоугольников\n",
    "\n",
    "    :param: predicted_bbox - [x_min, y_min, x_max, y_max]\n",
    "    :param: gt_bbox - [x_min, y_min, x_max, y_max]\n",
    "    \n",
    "    :return: Intersection Over Union\n",
    "    \"\"\"\n",
    "\n",
    "    intersection_bbox = np.array(\n",
    "        [\n",
    "            max(predicted_bbox[0], gt_bbox[0]),\n",
    "            max(predicted_bbox[1], gt_bbox[1]),\n",
    "            min(predicted_bbox[2], gt_bbox[2]),\n",
    "            min(predicted_bbox[3], gt_bbox[3]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    intersection_area = max(intersection_bbox[2] - intersection_bbox[0], 0) * max(\n",
    "        intersection_bbox[3] - intersection_bbox[1], 0\n",
    "    )\n",
    "    area_dt = (predicted_bbox[2] - predicted_bbox[0]) * (predicted_bbox[3] - predicted_bbox[1])\n",
    "    area_gt = (gt_bbox[2] - gt_bbox[0]) * (gt_bbox[3] - gt_bbox[1])\n",
    "\n",
    "    union_area = area_dt + area_gt - intersection_area\n",
    "\n",
    "    iou = intersection_area / union_area\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVJWo3xbMpSh"
   },
   "source": [
    "Теперь начинается основная часть домашнего задания: обучите модель YOLO для object detection на __обучающем__ датасете. \n",
    "\n",
    " - Создайте модель и функцию ошибки YoloV1 прочитав [оригинальную статью](https://paperswithcode.com/paper/you-only-look-once-unified-real-time-object)\n",
    " - Напишите функцию обучения модели\n",
    " - Используйте аугментации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxfMVwzHW2MJ"
   },
   "source": [
    "## Реализуйте Модель - 2 балла\n",
    "\n",
    "Копировать точное количество слоев и параметры сверток необязательно. Главное - чтобы модель работала по принципу, описанному в статье и делала предсказание в представленном формате.\n",
    "\n",
    "\n",
    "В качестве подсказки напомним, что выходом модели __для каждого обьекта__ должен быть тензор размера\n",
    "__S * S * (B * 5 + С)__, где все параметры имеют такое же значение, как и в статье: \n",
    "\n",
    "- S - количество ячеек на которое разбивается изображение по вертикали/горизонтали\n",
    "- В - количество предсказываемых прямоугольников в каждой ячейке\n",
    "- 5 - количество параметров для определения каждого прямоугольника (x_center, y_center, width, height, confidence)\n",
    "- С - количество классов (apple, banana, orange)\n",
    "\n",
    "Таким образом, мы для каждого окна размера __S x S__ предсказываем __В__ коробо и один класс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "3PJwrvcWW1n7"
   },
   "outputs": [],
   "source": [
    "#(kernel_size, filters, stride, padding) \n",
    "architecture_config = [\n",
    "    (7, 64, 2, 3),\n",
    "    \"M\",\n",
    "    (3, 192, 1, 1),\n",
    "    \"M\",\n",
    "    (1, 128, 1, 0),\n",
    "    (3, 256, 1, 1),\n",
    "    (1, 256, 1, 0),\n",
    "    (3, 512, 1, 1),\n",
    "    \"M\",\n",
    "    [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n",
    "    (1, 512, 1, 0),\n",
    "    (3, 1024, 1, 1),\n",
    "    \"M\",\n",
    "    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 2, 1),\n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 1, 1),\n",
    "]\n",
    "\n",
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.leakyrelu(self.batchnorm(self.conv(x)))\n",
    "\n",
    "\n",
    "class Yolo(nn.Module):\n",
    "    def __init__(self, in_channels=3, **kwargs):\n",
    "        super(Yolo, self).__init__()\n",
    "        self.architecture = architecture_config\n",
    "        self.in_channels = in_channels\n",
    "        self.darknet = self._create_conv_layers(self.architecture)\n",
    "        self.fcs = self._create_fcs(**kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.darknet(x)\n",
    "        return self.fcs(torch.flatten(x, start_dim=1))\n",
    "\n",
    "    def _create_conv_layers(self, architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for x in architecture:\n",
    "            if type(x) == tuple:\n",
    "                layers += [\n",
    "                    CNNBlock(\n",
    "                        in_channels, x[1], kernel_size=x[0], stride=x[2], padding=x[3],\n",
    "                    )\n",
    "                ]\n",
    "                in_channels = x[1]\n",
    "\n",
    "            elif type(x) == str:\n",
    "                layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))]\n",
    "\n",
    "            elif type(x) == list:\n",
    "                conv1 = x[0]\n",
    "                conv2 = x[1]\n",
    "                num_repeats = x[2]\n",
    "\n",
    "                for _ in range(num_repeats):\n",
    "                    layers += [\n",
    "                        CNNBlock(\n",
    "                            in_channels,\n",
    "                            conv1[1],\n",
    "                            kernel_size=conv1[0],\n",
    "                            stride=conv1[2],\n",
    "                            padding=conv1[3],\n",
    "                        )\n",
    "                    ]\n",
    "                    layers += [\n",
    "                        CNNBlock(\n",
    "                            conv1[1],\n",
    "                            conv2[1],\n",
    "                            kernel_size=conv2[0],\n",
    "                            stride=conv2[2],\n",
    "                            padding=conv2[3],\n",
    "                        )\n",
    "                    ]\n",
    "                    in_channels = conv2[1]\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _create_fcs(self, split_size, num_boxes, num_classes):\n",
    "        S, B, C = split_size, num_boxes, num_classes\n",
    "\n",
    "        return nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024 * S * S, 496),\n",
    "            nn.Dropout(0.0),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(496, S * S * (C + B * 5)),\n",
    "        )\n",
    "\"\"\"\n",
    "def test(S = 7, B=2, C = 3):\n",
    "    temp_model = Yolo(split_size = S, num_boxes = B, num_classes = C)\n",
    "    x = torch.randn((1,3,448,448))\n",
    "    print(temp_model(train_dataset[0][0].float()).shape)\n",
    "       \n",
    "\"\"\"\n",
    "S = 7\n",
    "B = 2\n",
    "C = 3\n",
    "temp_model = Yolo(split_size = S, num_boxes = B, num_classes = C)\n",
    "expected_output_shape =  S* S * (5 * B +C)\n",
    "testing_image = train_dataset[0][0].float()\n",
    "\n",
    "assert temp_model(testing_image).reshape(-1).shape[0] == expected_output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([9, 2, 1, 3])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7b/_0j65qyd6_l801khvk_jh2yh0000gn/T/ipykernel_1666/3106204335.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  a = np.array([[0,1,2,3,3], [0,12,3,4], [9,2,1,3]])\n"
     ]
    }
   ],
   "source": [
    "#test()\n",
    "# 637 = 7*7(5*2+3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJIjWKbcYUYe"
   },
   "source": [
    "## Реализуйте YoloLoss - 3 балла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "TwJZ7o0BYTbB"
   },
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self, S=7, B=2, C=3):\n",
    "        \"\"\"\n",
    "        :param: S * S - количество ячеек на которые разбивается изображение\n",
    "        :param: B - количество предсказанных прямоугольников в каждой ячейке\n",
    "        :param: C - количество классов\n",
    "        \"\"\"\n",
    "        \n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        predictions = predictions.reshape(-1, self.S, self.C + self.B*5)\n",
    "\n",
    "        iou_b1 = intersection_over_union(predictions[..., 4:8], target[..., 4:8])\n",
    "        iou_b2 = intersection_over_union(predictions[..., 9:13], target[..., 9:13])\n",
    "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim = 0)\n",
    "        iou_maxes, bestbox = torch.max(ious, dim = 0)\n",
    "        exists_box = target[...,3].unsqueeze(3)\n",
    "\n",
    "\n",
    "        box_predictions = exists_box * ( \n",
    "            (\n",
    "                bestbox*predictions[..., 9:13] \n",
    "                + (1- bestbox)*predictions[..., 4:8]\n",
    "            )\n",
    "        )\n",
    "        box_targets = exists_box* target[..., 4:8]\n",
    "\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[...,2:4])*torch.sqrt(\n",
    "            torch.abs(box_predictions[..., 2:4] + 1e-6)\n",
    "        )\n",
    "\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[...,2:4])\n",
    "\n",
    "        box_loss = self.mse(torch.flatten(box_predictions, end_dim = -2),\n",
    "                            torch.flatten(box_targets, end_dim=-2)\n",
    "        )\n",
    "\n",
    "        pred_box =( bestbox * predictions[..., 8:9] + (1-bestbox)* predictions[...,4:5]\n",
    "        )\n",
    "\n",
    "        obj_loss = self.mse(\n",
    "            torch.flatten(exists_box*pred_box),\n",
    "            torch.flatten(exists_box * target[..., 4:5])\n",
    "        )\n",
    "\n",
    "\n",
    "        no_obj_loss = self.mse(\n",
    "            torch.flatten((1-exists_box)*predictions[...,4:5], start_dim=1),\n",
    "            torch.flatten((1-exists_box) * target[...,4:5], start_dim=1)\n",
    "        )\n",
    "\n",
    "\n",
    "        no_obj_loss += self.mse(\n",
    "            torch.flatten((1-exists_box)*predictions[...,8:9], start_dim=1),\n",
    "            torch.flatten((1-exists_box) * target[...,4:5], start_dim=1)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box * predictions[..., :13], end_dim = -2),\n",
    "            torch.flatten(exists_box * target[..., :13], end_dim = -2)\n",
    "        )\n",
    "\n",
    "        loss = ( \n",
    "            self.lambda_coord *box_loss \n",
    "            + obj_loss\n",
    "            + self.lambda_noobj * no_obj_loss\n",
    "            + class_loss\n",
    "        )\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZ1eev1EeNk7"
   },
   "source": [
    "## Реализуйте дополнительные функции из статьи - 2 балла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(bboxes, iou_threshold, threshold):\n",
    "\n",
    "    assert type(bboxes) == list\n",
    "\n",
    "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "    bboxes_after_nms = []\n",
    "\n",
    "    while bboxes:\n",
    "        chosen_box = bboxes.pop(0)\n",
    "# compare bboxs in each class\n",
    "        bboxes = [\n",
    "            box\n",
    "            for box in bboxes\n",
    "            if box[0] != chosen_box[0]\n",
    "            or intersection_over_union(\n",
    "                torch.tensor(chosen_box[2:]),\n",
    "                torch.tensor(box[2:])\n",
    "            )\n",
    "            < iou_threshold\n",
    "        ]\n",
    "\n",
    "        bboxes_after_nms.append(chosen_box)\n",
    "\n",
    "    return bboxes_after_nms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bboxes(\n",
    "    loader,\n",
    "    model,\n",
    "    iou_threshold=.5,\n",
    "    threshold=.4,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    all_pred_boxes = []\n",
    "    all_true_boxes = []\n",
    "\n",
    "    model.eval()\n",
    "    train_idx = 0\n",
    "\n",
    "    for batch_idx, (x, labels) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(x)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        true_bboxes = cellboxes_to_boxes(labels)\n",
    "        bboxes = cellboxes_to_boxes(predictions)\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            nms_boxes = non_max_suppression(\n",
    "                bboxes[idx],\n",
    "                iou_threshold=iou_threshold,\n",
    "                threshold=threshold\n",
    "            )\n",
    "\n",
    "\n",
    "            for nms_box in nms_boxes:\n",
    "                all_pred_boxes.append([train_idx] + nms_box)\n",
    "\n",
    "            for box in true_bboxes[idx]:\n",
    "                if box[1] > threshold:\n",
    "                    all_true_boxes.append([train_idx] + box)\n",
    "\n",
    "            train_idx += 1\n",
    "\n",
    "    model.train()\n",
    "    return all_pred_boxes, all_true_boxes\n",
    "\n",
    "\n",
    "\n",
    "def cellboxes_to_boxes(out, S=7):\n",
    "    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n",
    "    converted_pred[..., 0] = converted_pred[..., 0].long()\n",
    "    all_bboxes = []\n",
    "\n",
    "    for ex_idx in range(out.shape[0]):\n",
    "        bboxes = []\n",
    "\n",
    "        for bbox_idx in range(S * S):\n",
    "            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n",
    "        all_bboxes.append(bboxes)\n",
    "\n",
    "        return all_bboxes\n",
    "\n",
    "\n",
    "def convert_cellboxes(predictions, S=7):\n",
    "    \"\"\"\n",
    "    Converts bounding boxes output from Yolo with\n",
    "    an image split size of S into entire image ratios\n",
    "    rather than relative to cell ratios.\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = predictions.to(\"cpu\")\n",
    "    batch_size = predictions.shape[0]\n",
    "    predictions = predictions.reshape(batch_size, 7, 7, 13)\n",
    "    bboxes1 = predictions[..., 4:8]\n",
    "    bboxes2 = predictions[..., 9:13]\n",
    "    scores = torch.cat(\n",
    "        (predictions[..., 3].unsqueeze(0), predictions[...,8].unsqueeze(0)), dim=0\n",
    "    )\n",
    "    best_box = scores.argmax(0).unsqueeze(-1)\n",
    "    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n",
    "    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n",
    "    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n",
    "    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n",
    "    w_y = 1 / S * best_boxes[..., 2:4]\n",
    "    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n",
    "    predicted_class = predictions[..., :3].argmax(-1).unsqueeze(-1)\n",
    "    best_confidence = torch.max(predictions[..., 3], predictions[..., 8]).unsqueeze(\n",
    "        -1\n",
    "    )\n",
    "    converted_preds = torch.cat(\n",
    "        (predicted_class, best_confidence, converted_bboxes), dim=-1\n",
    "    )\n",
    "\n",
    "    return converted_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precision(\n",
    "    pred_boxes,\n",
    "    true_boxes,\n",
    "    iou_threshold=.5,\n",
    "    num_classes=3\n",
    "):\n",
    "\n",
    "    # list storing all AP for respective classes\n",
    "    average_precisions = []\n",
    "\n",
    "    # used for numerical stability later on\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "\n",
    "        # Go through all predictions and targets,\n",
    "        # and only add the ones that belong to the\n",
    "        # current class c\n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c:\n",
    "                detections.append(detection)\n",
    "\n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1] == c:\n",
    "                ground_truths.append(true_box)\n",
    "\n",
    "        # find the amount of bboxes for each training example\n",
    "        # Counter here finds how many ground truth bboxes we get\n",
    "        \n",
    "        # amount_bboxes = {0:3, 1:5}\n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "\n",
    "\n",
    "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        # sort by box probabilities which is index 2\n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "        \n",
    "        # If none exists for this class then we can safely skip\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            # Only take out the ground_truths that have the same\n",
    "            # training idx as detection\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "\n",
    "            num_gts = len(ground_truth_img)\n",
    "            best_iou = 0\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = intersection_over_union(\n",
    "                    torch.tensor(detection[3:]),\n",
    "                    torch.tensor(gt[3:])\n",
    "                )\n",
    "\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            if best_iou > iou_threshold:\n",
    "                # only detect ground truth detection once\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    # true positive and add this bounding box to seen\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    FP[detection_idx] = 1\n",
    "\n",
    "            # if IOU is lower then the detection is a false positive\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "    return sum(average_precisions) / len(average_precisions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z38hYLM6haDk"
   },
   "source": [
    "## Обучите модель и посчитайте метрики для задачи детекции - 2 балла \n",
    "\n",
    "Несмотря на то, что в этом блоке ничего сильно нового для вас не ожидается и за него формально дается лишь два балла - провести обучение очень важно для понимания того, насколько правильно реализована ваша модель и лосс.\n",
    "\n",
    "В процессе обучения будет видно все ли размерности совпадают, падает ли лосс и растут ли метрики целевой задачи, поэтому на практике этот пункт гораздо оказывается гораздо важнее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(boxes, labels,S=7, B=2, C=3):\n",
    " label_matrix = torch.zeros((S, S, C + 5 * B))\n",
    " for box,i in enumerate(boxes):\n",
    "    x,y,w,h = box.tolist()\n",
    "    class_label = labels[i]\n",
    "    class_label = int(class_label)\n",
    "\n",
    "    i, j = int(S * y), int(S * x)\n",
    "    x_cell, y_cell = S * x - j, S * y - i\n",
    "    width_cell, height_cell = (w * S, h * S)\n",
    "\n",
    "    # If no object already found for specific cell i,j\n",
    "    # Note: This means we restrict to ONE object\n",
    "    # per cell!\n",
    "    if label_matrix[i, j, C] == 0:\n",
    "        # Set that there exists an object\n",
    "        label_matrix[i, j, C] = 1\n",
    "\n",
    "        # Box coordinates\n",
    "        box_coordinates = torch.tensor(\n",
    "            [x_cell, y_cell, width_cell, height_cell]\n",
    "        )\n",
    "\n",
    "        label_matrix[i, j, C+1:C+5] = box_coordinates\n",
    "\n",
    "        # Set one hot encoding for class_label\n",
    "        label_matrix[i, j, class_label] = 1\n",
    "\n",
    "        return label_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c этим не успел разобраться\n",
    "\n",
    "def train_fn(train_loader, model, optimizer, loss_fn, device = \"gpu\" ):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    mean_loss = []\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, y)\n",
    "        mean_loss.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update progress bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Mean loss {sum(mean_loss)/len(mean_loss)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS= 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = Yolo(split_size=7, num_boxes=2, num_classes=3).to(device)\n",
    "optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "loss_fn = YoloLoss()\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "        pred_boxes, target_boxes = get_bboxes(\n",
    "            train_dataloader, model, iou_threshold=0.5, threshold=0.4\n",
    "        )\n",
    "\n",
    "        mean_avg_prec = mean_average_precision(\n",
    "            pred_boxes, target_boxes, iou_threshold=0.5\n",
    "        )\n",
    "        print(f\"Train mAP: {mean_avg_prec}\")\n",
    "\n",
    "        train_fn(train_dataloader, model, optimizer, loss_fn)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
